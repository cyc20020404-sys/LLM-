import os
import re
import sys
from threading import Thread

# ä¼˜å…ˆä½¿ç”¨ç³»ç»Ÿ libstdc++ï¼Œé¿å… Conda ä¸‹ llama-cpp-python æŠ¥ GLIBCXX_3.4.30 not foundï¼ˆé¡»åœ¨ import llama_cpp ä¹‹å‰ï¼‰
for _path in ("/usr/lib/x86_64-linux-gnu", "/usr/lib64"):
    if os.path.isdir(_path):
        _prev = os.environ.get("LD_LIBRARY_PATH", "")
        os.environ["LD_LIBRARY_PATH"] = _path + (":" + _prev if _prev else "")
        break

import streamlit as st
import torch
import time

def _log(msg):
    """è¾“å‡ºåˆ°ç»ˆç«¯ï¼Œä¾¿äºåœ¨æ— æµè§ˆå™¨æ—¥å¿—æ—¶æ’æŸ¥"""
    print(f"[streamlit æ¨¡å‹] {msg}", flush=True)

# Unsloth ä»…åœ¨åŠ è½½ HF æ¨¡å‹æ—¶æŒ‰éœ€å¯¼å…¥ï¼ˆéœ€ GPUï¼‰ï¼›ä½¿ç”¨ GGUF æ—¶ä¸å¯¼å…¥ï¼Œé¿å…æ—  GPU ç¯å¢ƒæŠ¥é”™

# é…ç½®ç¯å¢ƒ
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
os.environ["HF_HUB_ENABLE_HF_TRANSFER"] = "0"

# æœ¬åœ°å¾®è°ƒæ¨¡å‹ï¼šä¼˜å…ˆ GGUF å•æ–‡ä»¶ï¼Œå…¶æ¬¡ä¸º HF æ ¼å¼ç›®å½•
_APP_DIR = os.path.dirname(os.path.abspath(__file__))
# GGUF å•æ–‡ä»¶ï¼ˆå¯é…ç½®å¤šä¸ª .gguf è·¯å¾„ï¼ŒæŒ‰é¡ºåºå°è¯•ï¼‰
LOCAL_GGUF_FILES = [
    os.path.join(_APP_DIR, "my_emotional_bot.Q4_K_M.gguf"),
]
# HF æ ¼å¼ç›®å½•ï¼ˆåˆå¹¶åçš„ safetensors ç­‰ï¼‰
LOCAL_MODEL_DIRS = [
    os.path.join(_APP_DIR, "merged_model"),
]

# ä¸‰å¥—è¯­æ°”æç¤ºè¯ï¼šå°å­© / å¹´è½»äºº / è€å¹´äºº
TONE_PROMPTS = {
    "å°å­©": "ã€å½“å‰å¯¹è¯å¯¹è±¡ï¼šå°æœ‹å‹ã€‘è¯·ç”¨å¯¹å°å­©è¯´è¯çš„æ–¹å¼å›ç­”ï¼šç”¨è¯ç®€å•ã€å¥å­çŸ­ï¼Œè¯­æ°”æ¸©æš–è€å¿ƒã€å¸¦ä¸€ç‚¹é¼“åŠ±å’Œè¶£å‘³ï¼Œé¿å…å¤æ‚æ¦‚å¿µï¼Œå¯ä»¥é€‚å½“ç”¨å è¯æˆ–æ‹Ÿå£°è¯ï¼Œè®©å¯¹è¯æ›´äº²åˆ‡å¯çˆ±ã€‚",
    "å¹´è½»äºº": "ã€å½“å‰å¯¹è¯å¯¹è±¡ï¼šå¹´è½»äººã€‘è¯·ç”¨è½»æ¾ã€è‡ªç„¶ã€åƒæœ‹å‹èŠå¤©çš„è¯­æ°”å›ç­”ï¼šç›´æ¥ä¸å•°å—¦ï¼Œå¯ä»¥å¸¦ä¸€ç‚¹æ—¥å¸¸æˆ–ç½‘ç»œç”¨è¯­ï¼Œä¿æŒå‹å¥½å’Œå…±é¸£ï¼ŒåƒåŒé¾„äººä¸€æ ·äº¤æµã€‚",
    "è€å¹´äºº": "ã€å½“å‰å¯¹è¯å¯¹è±¡ï¼šé•¿è¾ˆ/è€å¹´äººã€‘è¯·ç”¨å°Šæ•¬ã€è€å¿ƒã€ä½“è´´çš„æ–¹å¼å›ç­”ï¼šæŠŠè¯è¯´æ¸…æ¥šã€è¯­é€Ÿæ”¾æ…¢çš„æ„Ÿè§‰ï¼Œå¤šç”¨æ•¬è¯­å’Œå…³å¿ƒï¼Œé¿å…ç½‘ç»œç”¨è¯­å’Œå¤æ‚æ¦‚å¿µï¼Œè®©å¯¹æ–¹æ„Ÿåˆ°è¢«å°Šé‡å’Œç…§é¡¾ã€‚",
}


def _build_chat_prompt(user_input: str) -> str:
    """æ ¹æ®å½“å‰é€‰æ‹©çš„è¯­æ°”é£æ ¼ï¼Œæ‹¼æ¥å¸¦è¯­æ°”æŒ‡ä»¤çš„å¯¹è¯ promptã€‚"""
    tone = st.session_state.get("tone_style", "å¹´è½»äºº")
    instruction = TONE_PROMPTS.get(tone, TONE_PROMPTS["å¹´è½»äºº"])
    return f"{instruction}\n\nUser: {user_input}\nAssistant:"


# åŠ è½½æ¨¡å‹ï¼ˆé¡»åœ¨ä¾§è¾¹æ â€œé¢„åŠ è½½æ¨¡å‹â€æŒ‰é’®ä¹‹å‰å®šä¹‰ï¼‰
@st.cache_resource
def load_model(model_type):
    """åŠ è½½æ¨¡å‹ã€‚æœ¬åœ°å¾®è°ƒï¼šä¼˜å…ˆ GGUF å•æ–‡ä»¶ï¼Œå¦åˆ™ HF ç›®å½•ï¼›ä¸åŠ è½½åŸºç¡€æ¨¡å‹ã€‚"""
    max_seq_length = 4096
    _log(f"å¼€å§‹åŠ è½½æ¨¡å‹ï¼Œç±»å‹: {model_type}")
    with st.spinner("ğŸ”„ æ­£åœ¨åŠ è½½æ¨¡å‹ï¼Œè¯·ç¨å€™..."):
        try:
            if model_type == "æœ¬åœ°å¾®è°ƒæ¨¡å‹":
                # 1) ä¼˜å…ˆï¼šGGUF å•æ–‡ä»¶
                for path in LOCAL_GGUF_FILES:
                    if not os.path.isfile(path):
                        _log(f"GGUF ä¸å­˜åœ¨ï¼Œè·³è¿‡: {path}")
                        continue
                    _log(f"å°è¯•åŠ è½½ GGUF: {path}")
                    try:
                        from llama_cpp import Llama
                        llm = Llama(
                            model_path=path,
                            n_ctx=max_seq_length,
                            n_gpu_layers=-1,
                            verbose=False,
                        )
                        _log(f"GGUF åŠ è½½æˆåŠŸ: {path}")
                        # æ£€æµ‹æ˜¯å¦ä¸º CUDA ç‰ˆï¼ˆå½“å‰è‹¥ä¸º CPU ç‰ˆä¼šå¯¼è‡´ GPU 0%ã€CPU 100% å¾ˆæ…¢ï¼‰
                        try:
                            from llama_cpp.llama_cpp import _load_shared_library
                            _lib = _load_shared_library("llama")
                            if getattr(_lib, "llama_supports_gpu_offload", lambda: False)():
                                _log("å½“å‰ llama-cpp-python æ”¯æŒ GPU å¸è½½ï¼Œæ¨ç†åº”èµ° GPU")
                            else:
                                _log("å½“å‰ä¸º CPU ç‰ˆ llama-cpp-pythonï¼Œæ¨ç†ä¼šéå¸¸æ…¢ï¼›è¯·å®‰è£… CUDA ç‰ˆï¼ˆè§ READMEï¼‰")
                                st.warning("âš ï¸ å½“å‰ä¸º **CPU ç‰ˆ** llama-cpp-pythonï¼Œæ¨ç†æ—¶ GPU ä¼šæ˜¾ç¤º 0%ã€CPU æ»¡è´Ÿè½½å¾ˆæ…¢ã€‚è¯·å®‰è£… CUDA ç‰ˆåé‡å¯åº”ç”¨ï¼Œè§ READMEã€Œè®© GGUF ä½¿ç”¨ 5090 æ˜¾å¡ã€ã€‚")
                        except Exception:
                            pass
                        st.success(f"âœ… æœ¬åœ° GGUF æ¨¡å‹åŠ è½½æˆåŠŸï¼š{path}")
                        return llm, None, "gguf"
                    except ImportError as e:
                        _log(f"ImportError: {e}")
                        st.error("âŒ è¯·å…ˆå®‰è£… llama-cpp-pythonï¼špip install llama-cpp-pythonï¼ˆGPU ç‰ˆéœ€å¸¦ CUDA ç¼–è¯‘ï¼‰")
                        return None, None, None
                    except Exception as e:
                        err = str(e)
                        _log(f"GGUF åŠ è½½å¼‚å¸¸ {path}: {err}")
                        if "not within the file bounds" in err or "corrupted or incomplete" in err:
                            st.warning(f"âš ï¸ GGUF æ–‡ä»¶å·²æŸåæˆ–ä¸å®Œæ•´ï¼Œè¯·é‡æ–°å¯¼å‡ºæˆ–ä¸‹è½½ï¼š{path}")
                        else:
                            st.warning(f"âš ï¸ GGUF {path} åŠ è½½å¤±è´¥: {err}")
                        continue
                # 2) å¤‡é€‰ï¼šHF æ ¼å¼ç›®å½•ï¼ˆéœ€ GPU + Unslothï¼‰
                for path in LOCAL_MODEL_DIRS:
                    if not os.path.exists(path):
                        _log(f"HF ç›®å½•ä¸å­˜åœ¨ï¼Œè·³è¿‡: {path}")
                        continue
                    _log(f"å°è¯•åŠ è½½ HF ç›®å½•: {path}")
                    try:
                        from unsloth import FastLanguageModel
                        model, tokenizer = FastLanguageModel.from_pretrained(
                            path,
                            max_seq_length=max_seq_length,
                            load_in_4bit=False,
                        )
                        model = FastLanguageModel.for_inference(model)
                        _log(f"HF åŠ è½½æˆåŠŸ: {path}")
                        st.success(f"âœ… æœ¬åœ°å¾®è°ƒæ¨¡å‹åŠ è½½æˆåŠŸï¼š{path}")
                        return model, tokenizer, "hf"
                    except Exception as e:
                        _log(f"HF åŠ è½½å¤±è´¥ {path}: {e}")
                        st.warning(f"âš ï¸ {path} åŠ è½½å¤±è´¥: {str(e)}")
                        continue
                _log("æœªæ‰¾åˆ°å¯ç”¨çš„æœ¬åœ°æ¨¡å‹")
                st.error("âŒ æœªæ‰¾åˆ°å¯ç”¨çš„æœ¬åœ°æ¨¡å‹ï¼ˆGGUF æ–‡ä»¶æˆ– merged_model ç­‰ç›®å½•ï¼‰ã€‚")
                return None, None, None

            # åŸºç¡€æ¨¡å‹ï¼šä» HuggingFace åŠ è½½ï¼ˆéœ€ GPU + Unslothï¼‰
            _log("åŠ è½½åŸºç¡€æ¨¡å‹ï¼ˆHuggingFaceï¼‰...")
            try:
                from unsloth import FastLanguageModel
                model_name = "unsloth/deepseek-r1-distill-qwen-7b-unsloth-bnb-4bit"
                model, tokenizer = FastLanguageModel.from_pretrained(
                    model_name=model_name,
                    max_seq_length=max_seq_length,
                    load_in_4bit=True,
                )
                model = FastLanguageModel.for_inference(model)
                _log("åŸºç¡€æ¨¡å‹åŠ è½½æˆåŠŸ")
                st.success("âœ… åŸºç¡€æ¨¡å‹åŠ è½½æˆåŠŸï¼")
                return model, tokenizer, "hf"
            except Exception as e:
                err = str(e)
                if "torch accelerator" in err.lower() or "need a gpu" in err.lower() or "cuda" in err.lower():
                    _log(f"åŸºç¡€æ¨¡å‹åŠ è½½å¤±è´¥ï¼ˆPyTorch æœªè§ GPUï¼‰: {e}")
                    cuda_visible = torch.cuda.is_available()
                    st.error(
                        "âŒ åŸºç¡€æ¨¡å‹ä¾èµ– **PyTorch** çš„ GPUï¼Œå½“å‰ PyTorch æœªæ£€æµ‹åˆ°å¯ç”¨ GPUã€‚"
                        "ï¼ˆGGUF ç”¨çš„æ˜¯ llama-cpp-python çš„ CUDAï¼Œå’Œ PyTorch æ— å…³ï¼Œæ‰€ä»¥å¾®è°ƒæ¨¡å‹èƒ½è·‘ã€‚ï¼‰\n\n"
                        "**è§£å†³åŠæ³•**ï¼šå®‰è£… PyTorch çš„ CUDA ç‰ˆåå†è¯•ã€‚AutoDL å¸¸è§ä¸º CUDA 12.8ï¼š\n"
                        "`pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128`"
                    )
                    return None, None, None
                raise

        except Exception as e:
            _log(f"æ¨¡å‹åŠ è½½å¼‚å¸¸: {e}")
            st.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
            return None, None, None


# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(
    page_title="AIé™ªä¼´æœºå™¨äºº",
    page_icon="ğŸ¤–",
    layout="wide"
)

# ç¡®ä¿ä¸»å†…å®¹åŒºå¯æ­£å¸¸ç”¨æ»šè½®ä¸Šä¸‹æ»šåŠ¨ï¼ˆä¸é”æ­»åº•éƒ¨ï¼‰
st.markdown(
    """
    <style>
    [data-testid="stAppViewContainer"] { overflow-y: auto !important; }
    .main .block-container { overflow-y: visible !important; }
    </style>
    """,
    unsafe_allow_html=True,
)

# å°½æ—©åˆå§‹åŒ– load_statusï¼Œé¿å…ä¸‹æ–¹ caption æŠ¥ AttributeError
if "load_status" not in st.session_state:
    st.session_state.load_status = "æœªåŠ è½½ï¼ˆå‘é€æ¶ˆæ¯æˆ–ç‚¹å‡»ã€Œé¢„åŠ è½½æ¨¡å‹ã€ååŠ è½½ï¼‰"

st.title("ğŸ¤– AIé™ªä¼´æœºå™¨äºº")
# æ¨¡å‹çŠ¶æ€ï¼ˆä¸»åŒºåŸŸæ˜¾ç¤ºï¼Œé¿å…æµè§ˆå™¨ AbortSignal ç­‰å¯¼è‡´ä¾§æ æç¤ºä¸æ˜¾ç¤ºï¼‰
st.caption("ğŸ“Œ æ¨¡å‹çŠ¶æ€ï¼š**" + st.session_state.load_status + "**")
st.markdown("---")

# ä¾§è¾¹æ é…ç½®
with st.sidebar:
    st.header("âš™ï¸ æ¨¡å‹é…ç½®")
    
    # è¯­æ°”é£æ ¼ï¼ˆå¯¹è¯å¯¹è±¡ï¼‰ï¼šå°å­© / å¹´è½»äºº / è€å¹´äºº
    tone_style = st.selectbox(
        "è¯­æ°”é£æ ¼ï¼ˆå¯¹è¯å¯¹è±¡ï¼‰",
        options=["å°å­©", "å¹´è½»äºº", "è€å¹´äºº"],
        index=1,
        key="tone_style",
        help="é€‰æ‹©å¯¹è¯å¯¹è±¡åï¼Œå°å›¢å›¢ä¼šç”¨å¯¹åº”è¯­æ°”å›ç­”ï¼ˆç”¨è¯ä¸é£æ ¼ä¼šéšä¹‹è°ƒæ•´ï¼‰",
    )
    
    # PyTorch æ˜¯å¦å¯è§ GPUï¼ˆåŸºç¡€æ¨¡å‹ä¾èµ– PyTorchï¼›GGUF ç”¨ llama-cpp-python çš„ CUDAï¼ŒäºŒè€…ç‹¬ç«‹ï¼‰
    _cuda_ok = torch.cuda.is_available()
    _cuda_msg = f"æ˜¯ï¼ˆ{torch.cuda.get_device_name(0)}ï¼‰" if _cuda_ok else "å¦"
    st.caption(f"ğŸ”§ PyTorch å¯è§ GPU: **{_cuda_msg}**")
    if not _cuda_ok:
        st.caption("ç³»ç»Ÿæœ‰ CUDA ä½† PyTorch å¯èƒ½æ˜¯ CPU ç‰ˆï¼Œéœ€é‡è£… cu128 ç‰ˆï¼›GGUF ä¸å—å½±å“")
    
    # æ¨¡å‹é€‰æ‹©
    model_choice = st.selectbox(
        "é€‰æ‹©æ¨¡å‹åŠ è½½æ–¹å¼",
        ["æœ¬åœ°å¾®è°ƒæ¨¡å‹", "åŸºç¡€æ¨¡å‹"],
        help="åŸºç¡€æ¨¡å‹éœ€ GPUï¼›æ—  GPU æ—¶è¯·é€‰ã€Œæœ¬åœ°å¾®è°ƒæ¨¡å‹ã€å¹¶åŠ è½½ GGUFã€‚",
    )
    if model_choice == "åŸºç¡€æ¨¡å‹":
        st.caption("âš ï¸ éœ€ PyTorch èƒ½è§ GPUï¼Œå¦åˆ™ä¼šåŠ è½½å¤±è´¥")
    
    # ç”Ÿæˆå‚æ•°é…ç½®
    col1, col2 = st.columns(2)
    with col1:
        max_tokens = st.slider(
            "æœ€å¤§ç”Ÿæˆé•¿åº¦",
            min_value=50,
            max_value=2048,
            value=500,
            step=50
        )
    
    with col2:
        temperature = st.slider(
            "æ¸©åº¦(Temperature)",
            min_value=0.0,
            max_value=2.0,
            value=0.7,
            step=0.1
        )
    
    top_p = st.slider(
        "Top P",
        min_value=0.0,
        max_value=1.0,
        value=0.9,
        step=0.05
    )
    
    st.markdown("---")
    # é¢„åŠ è½½æŒ‰é’®ï¼šå…ˆåŠ è½½æ¨¡å‹å¹¶çœ‹ç»ˆç«¯æ—¥å¿—ï¼Œé¿å…â€œå‘æ¶ˆæ¯æ‰åŠ è½½â€ä¸”å‰ç«¯ä¸æ˜¾ç¤ºçš„é—®é¢˜
    if st.button("ğŸ”„ é¢„åŠ è½½æ¨¡å‹", use_container_width=True, type="primary"):
        _log(f"ç”¨æˆ·ç‚¹å‡»é¢„åŠ è½½ï¼Œå½“å‰é€‰æ‹©: {model_choice}")
        model, tokenizer, backend = load_model(model_choice)
        st.session_state.model = model
        st.session_state.tokenizer = tokenizer
        st.session_state.backend = backend or "hf"
        st.session_state.model_loaded = True
        st.session_state.current_model = model_choice
        if model is not None:
            st.session_state.load_status = f"âœ… å·²åŠ è½½ï¼ˆ{model_choice}ï¼‰"
            _log("é¢„åŠ è½½æˆåŠŸ")
        else:
            st.session_state.load_status = "âŒ åŠ è½½å¤±è´¥ï¼Œè¯·æŸ¥çœ‹ç»ˆç«¯æ—¥å¿—"
            _log("é¢„åŠ è½½å¤±è´¥")
    st.markdown("---")
    st.markdown("**ğŸ’¡ å‚æ•°è¯´æ˜:**")
    st.markdown("""
    - **æ¸©åº¦**: å€¼è¶Šé«˜è¶Šå…·æœ‰åˆ›æ„ï¼Œè¶Šä½è¶Šä¿å®ˆ
    - **Top P**: æ§åˆ¶ç”Ÿæˆå¤šæ ·æ€§çš„å‚æ•°
    """)


# åˆå§‹åŒ–session state
if "messages" not in st.session_state:
    st.session_state.messages = []
    st.session_state.model_loaded = False
    st.session_state.model = None
    st.session_state.tokenizer = None
    st.session_state.current_model = None
    st.session_state.backend = "hf"  # "hf" | "gguf"


def _strip_think_tags(text: str) -> str:
    """åªä¿ç•™ </think> ä¹‹åçš„å›ç­”éƒ¨åˆ†ï¼Œå»æ‰ <think>...</think> æ€è€ƒå†…å®¹åŠæœ«å°¾è‡ªæ£€å¥ï¼ˆå¦‚ã€Œè¯·é—®ï¼Œè¿™ä¸ªå›åº”æ˜¯å¦ç¬¦åˆè¦æ±‚ï¼Ÿã€ï¼‰ã€‚"""
    if not text:
        return ""
    # æŒ‰ç¬¬ä¸€ä¸ª </think> æˆªæ–­ï¼Œåªä¿ç•™åé¢çš„æ­£å¼å›ç­”
    if "</think>" in text:
        out = text.split("</think>", 1)[-1].strip()
    else:
        out = re.sub(r"<think>.*", "", text, flags=re.DOTALL).strip()
    # è‹¥å›ç­”é‡Œåˆå‡ºç° <think>ï¼Œåªä¿ç•™ç¬¬ä¸€æ®µï¼ˆåˆ°ä¸‹ä¸€ä¸ª <think> æˆ– </think> ä¹‹å‰ï¼‰
    if "<think>" in out:
        out = re.sub(r"<think>.*", "", out, flags=re.DOTALL).strip()
    # å»æ‰æœ«å°¾æ¨¡å‹è‡ªæ£€å¥
    out = re.sub(r"è¯·é—®[ï¼Œ,]?\s*è¿™ä¸ªå›åº”æ˜¯å¦ç¬¦åˆè¦æ±‚[ï¼Ÿ?].*$", "", out, flags=re.DOTALL).strip()
    return out


def stream_gguf_response(user_input, placeholder, max_tok, temp, top_p_val):
    """GGUF æµå¼ç”Ÿæˆï¼Œè¾¹ç”Ÿæˆè¾¹æ›´æ–° placeholderï¼Œè¿”å›å®Œæ•´å›å¤ã€‚åªæ˜¾ç¤º </think> åçš„å›ç­”ã€‚"""
    prompt = _build_chat_prompt(user_input)
    full = ""
    try:
        stream = st.session_state.model(
            prompt,
            max_tokens=max_tok,
            temperature=temp,
            top_p=top_p_val,
            repeat_penalty=1.15,
            stop=["User:", "\nUser:", "å°å›¢å›¢'ã€‚ã€‚"],
            echo=False,
            stream=True,
        )
        for chunk in stream:
            piece = (chunk.get("choices") or [{}])[0].get("text") or ""
            full += piece
            # åªæŠŠ </think> åçš„å†…å®¹å±•ç¤ºç»™ç”¨æˆ·ï¼Œé¿å…éœ²å‡ºæ€è€ƒè¿‡ç¨‹
            to_show = _strip_think_tags(full)
            placeholder.markdown(to_show + "â–Œ")
        to_show = _strip_think_tags(full)
        placeholder.markdown(to_show)
        if "Assistant:" in full:
            full = full.split("Assistant:")[-1].strip()
        full = _strip_think_tags(full)
        return full.strip() or "ï¼ˆæ— è¾“å‡ºï¼‰"
    except Exception as e:
        placeholder.markdown(f"âŒ ç”Ÿæˆå¤±è´¥: {str(e)}")
        return f"âŒ ç”Ÿæˆå¤±è´¥: {str(e)}"


def stream_hf_response(user_input, placeholder, max_tok, temp, top_p_val):
    """HF åŸºç¡€æ¨¡å‹æµå¼ç”Ÿæˆï¼Œè¾¹ç”Ÿæˆè¾¹æ›´æ–° placeholderï¼Œåªæ˜¾ç¤º </think> åçš„å›ç­”ã€‚"""
    if st.session_state.model is None or st.session_state.tokenizer is None:
        placeholder.markdown("âŒ æ¨¡å‹æœªåŠ è½½æˆ–çŠ¶æ€å¼‚å¸¸")
        return "âŒ æ¨¡å‹æœªåŠ è½½"
    try:
        from transformers import TextIteratorStreamer
        prompt = _build_chat_prompt(user_input)
        device = "cuda" if torch.cuda.is_available() else "cpu"
        inputs = st.session_state.tokenizer(
            prompt, return_tensors="pt", truncation=True, max_length=4096
        ).to(device)
        streamer = TextIteratorStreamer(
            st.session_state.tokenizer, skip_special_tokens=True
        )
        gen_kwargs = dict(
            **inputs,
            max_new_tokens=max_tok,
            temperature=temp,
            top_p=top_p_val,
            do_sample=True,
            pad_token_id=st.session_state.tokenizer.pad_token_id,
            eos_token_id=st.session_state.tokenizer.eos_token_id,
            streamer=streamer,
        )
        thread = Thread(target=st.session_state.model.generate, kwargs=gen_kwargs)
        thread.start()
        full = ""
        for new_text in streamer:
            full += new_text
            placeholder.markdown(_strip_think_tags(full) + "â–Œ")
        thread.join()
        full = _strip_think_tags(full)
        placeholder.markdown(full)
        if "Assistant:" in full:
            full = full.split("Assistant:")[-1].strip()
        full = _strip_think_tags(full)
        return full.strip() or "ï¼ˆæ— è¾“å‡ºï¼‰"
    except Exception as e:
        placeholder.markdown(f"âŒ ç”Ÿæˆå¤±è´¥: {str(e)}")
        return f"âŒ ç”Ÿæˆå¤±è´¥: {str(e)}"


def generate_response(user_input):
    """ç”Ÿæˆæ¨¡å‹å“åº”ï¼ˆæ”¯æŒ HF ä¸ GGUF ä¸¤ç§åç«¯ï¼‰"""
    if st.session_state.model is None:
        return "âŒ æ¨¡å‹æœªåŠ è½½ï¼Œè¯·å…ˆåŠ è½½æ¨¡å‹"
    backend = st.session_state.get("backend", "hf")

    try:
        prompt = _build_chat_prompt(user_input)

        if backend == "gguf":
            # GGUF ä½¿ç”¨æµå¼åœ¨å¤–éƒ¨è°ƒç”¨ï¼Œæ­¤å¤„ä»…ä½œéæµå¼å…œåº•ï¼ˆä¸€èˆ¬ä¸èµ°åˆ°ï¼‰
            with st.spinner("ğŸ¤” æ¨¡å‹æ€è€ƒä¸­..."):
                out = st.session_state.model(
                    prompt,
                    max_tokens=min(max_tokens, 256),
                    temperature=temperature,
                    top_p=top_p,
                    repeat_penalty=1.15,
                    stop=["User:", "\nUser:", "å°å›¢å›¢'ã€‚ã€‚"],
                    echo=False,
                )
                response = (out["choices"][0].get("text") or "").strip()
                if "Assistant:" in response:
                    response = response.split("Assistant:")[-1].strip()
                response = _strip_think_tags(response)
                return response or "ï¼ˆæ— è¾“å‡ºï¼‰"

        # HF åç«¯
        if st.session_state.tokenizer is None:
            return "âŒ æ¨¡å‹çŠ¶æ€å¼‚å¸¸"
        inputs = st.session_state.tokenizer.encode(
            prompt, return_tensors="pt"
        ).to("cuda" if torch.cuda.is_available() else "cpu")
        with torch.no_grad():
            outputs = st.session_state.model.generate(
                inputs,
                max_new_tokens=max_tokens,
                temperature=temperature,
                top_p=top_p,
                do_sample=True,
                pad_token_id=st.session_state.tokenizer.pad_token_id,
                eos_token_id=st.session_state.tokenizer.eos_token_id,
            )
        response = st.session_state.tokenizer.decode(
            outputs[0], skip_special_tokens=True
        )
        if "Assistant:" in response:
            response = response.split("Assistant:")[-1].strip()
        return response
    except Exception as e:
        return f"âŒ ç”Ÿæˆå¤±è´¥: {str(e)}"


# ä¸»ç•Œé¢
col1, col2 = st.columns([3, 1])

with col1:
    st.subheader("ğŸ’¬ å¯¹è¯åŒºåŸŸ")

with col2:
    if st.button("ğŸ”„ æ¸…ç©ºå¯¹è¯", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

# æ˜¾ç¤ºå¯¹è¯å†å²
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ã€Œå›åˆ°åº•éƒ¨ã€æŒ‰é’®ï¼šä»…ç‚¹å‡»æ—¶æ‰§è¡Œä¸€æ¬¡æ»šåŠ¨ï¼Œä¸å¹²æ‰°å¹³æ—¶ç”¨æ»šè½®ä¸Šä¸‹ç¿»çœ‹
if st.session_state.get("do_auto_scroll", False):
    try:
        from streamlit.components.v1 import html as st_html
        st_html(
            """
            <script>
            (function() {
                var t = window.parent;
                if (!t || t === window) t = window;
                function run() {
                    try {
                        var app = t.document.querySelector('[data-testid="stAppViewContainer"]');
                        if (app && app.scrollHeight > app.clientHeight) {
                            app.scrollTop = app.scrollHeight;
                        }
                        var main = t.document.querySelector('.main');
                        if (main && main.scrollHeight > main.clientHeight) {
                            main.scrollTop = main.scrollHeight;
                        }
                        t.scrollTo(0, t.document.body.scrollHeight);
                    } catch (e) {}
                }
                setTimeout(run, 100);
            })();
            </script>
            """,
            height=0,
        )
    except Exception:
        pass
    st.session_state.do_auto_scroll = False

with col2:
    if st.button("â¬‡ï¸ å›åˆ°åº•éƒ¨", use_container_width=True, help="æ»šåŠ¨åˆ°æœ€æ–°ä¸€æ¡æ¶ˆæ¯"):
        st.session_state.do_auto_scroll = True
        st.rerun()

# è¾“å…¥æ¡†
user_input = st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜æˆ–æƒ³è¯´çš„è¯...")

if user_input:
    # åŠ è½½æ¨¡å‹ï¼ˆå¦‚æœè¿˜æœªåŠ è½½ï¼‰
    if not st.session_state.model_loaded or st.session_state.current_model != model_choice:
        model, tokenizer, backend = load_model(model_choice)
        st.session_state.model = model
        st.session_state.tokenizer = tokenizer
        st.session_state.backend = backend or "hf"
        st.session_state.model_loaded = True
        st.session_state.current_model = model_choice
        st.session_state.load_status = f"âœ… å·²åŠ è½½ï¼ˆ{model_choice}ï¼‰" if model else "âŒ åŠ è½½å¤±è´¥ï¼Œè¯·æŸ¥çœ‹ç»ˆç«¯"
    
    # æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯
    st.session_state.messages.append({
        "role": "user",
        "content": user_input
    })
    
    with st.chat_message("user"):
        st.markdown(user_input)
    
    # ç”ŸæˆAIå“åº”
    if st.session_state.model is not None:
        backend = st.session_state.get("backend", "hf")
        if backend == "gguf":
            with st.chat_message("assistant"):
                placeholder = st.empty()
                response = stream_gguf_response(
                    user_input, placeholder,
                    max_tok=min(max_tokens, 512),
                    temp=temperature,
                    top_p_val=top_p,
                )
        else:
            # HF åŸºç¡€æ¨¡å‹ä¹Ÿæµå¼è¾“å‡º
            with st.chat_message("assistant"):
                placeholder = st.empty()
                response = stream_hf_response(
                    user_input, placeholder,
                    max_tok=min(max_tokens, 512),
                    temp=temperature,
                    top_p_val=top_p,
                )
        
        # ä¿å­˜åˆ°å¯¹è¯å†å²
        st.session_state.messages.append({
            "role": "assistant",
            "content": response
        })
    else:
        st.error("âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œæ— æ³•ç”Ÿæˆå“åº”")
    
    st.rerun()

# åº•éƒ¨ä¿¡æ¯
st.markdown("---")
st.markdown("""
<div style='text-align: center'>
    <small>ğŸš€ é©±åŠ¨æ¨¡å‹: DeepSeek R1 Distill Qwen 7B</small>
    <br>
    <small>âš¡ æ¡†æ¶: Unsloth + Streamlit</small>
</div>
""", unsafe_allow_html=True)
