import os
import re
import sys
from threading import Thread

# ä¼˜å…ˆä½¿ç”¨ç³»ç»Ÿ libstdc++ï¼Œé¿å… Conda ä¸‹ llama-cpp-python æŠ¥ GLIBCXX_3.4.30 not foundï¼ˆé¡»åœ¨ import llama_cpp ä¹‹å‰ï¼‰
for _path in ("/usr/lib/x86_64-linux-gnu", "/usr/lib64"):
    if os.path.isdir(_path):
        _prev = os.environ.get("LD_LIBRARY_PATH", "")
        os.environ["LD_LIBRARY_PATH"] = _path + (":" + _prev if _prev else "")
        break

import streamlit as st
import torch
import time

def _log(msg):
    """è¾“å‡ºåˆ°ç»ˆç«¯ï¼Œä¾¿äºåœ¨æ— æµè§ˆå™¨æ—¥å¿—æ—¶æ’æŸ¥"""
    print(f"[streamlit æ¨¡å‹] {msg}", flush=True)

# Unsloth ä»…åœ¨åŠ è½½ HF æ¨¡å‹æ—¶æŒ‰éœ€å¯¼å…¥ï¼ˆéœ€ GPUï¼‰ï¼›ä½¿ç”¨ GGUF æ—¶ä¸å¯¼å…¥ï¼Œé¿å…æ—  GPU ç¯å¢ƒæŠ¥é”™

# é…ç½®ç¯å¢ƒ
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
os.environ["HF_HUB_ENABLE_HF_TRANSFER"] = "0"

# æœ¬åœ°å¾®è°ƒæ¨¡å‹ï¼šä¼˜å…ˆ GGUF å•æ–‡ä»¶ï¼Œå…¶æ¬¡ä¸º HF æ ¼å¼ç›®å½•
_APP_DIR = os.path.dirname(os.path.abspath(__file__))
# GGUF å•æ–‡ä»¶ï¼ˆå¯é…ç½®å¤šä¸ª .gguf è·¯å¾„ï¼ŒæŒ‰é¡ºåºå°è¯•ï¼‰
LOCAL_GGUF_FILES = [
    os.path.join(_APP_DIR, "my_emotional_bot.Q4_K_M.gguf"),
]
# HF æ ¼å¼ç›®å½•ï¼ˆåˆå¹¶åçš„ safetensors ç­‰ï¼‰
LOCAL_MODEL_DIRS = [
    os.path.join(_APP_DIR, "merged_model"),
]
# DPO å¾®è°ƒæ¨¡å‹ï¼šç›´æ¥åŠ è½½ LoRA é€‚é…å™¨ï¼ˆåŸºç¡€æ¨¡å‹ + adapterï¼Œæ— éœ€åˆå¹¶ï¼‰
LOCAL_DPO_MODEL_DIR = os.path.join(_APP_DIR, "lora_model_dpo")

# è¯­æ°”æç¤ºè¯ï¼šéœ€ä¸ DPO äººè®¾ï¼ˆå°å›¢å›¢ã€æ´»æ³¼æ¸©æŸ”ï¼‰å…¼å®¹ï¼Œé¿å…æŒ‡ä»¤å†²çªå¯¼è‡´ä¹±è®²
TONE_PROMPTS = {
    "æ— æç¤ºè¯": "",
    "å°å­©": "ã€å°å›¢å›¢å¯¹å°æœ‹å‹ã€‘ç”¨è¯ç®€å•ã€å¥å­çŸ­ï¼Œè¯­æ°”æ¸©æš–è€å¿ƒï¼Œå¯å¶å°”ç”¨å è¯å¢åŠ è¶£å‘³ï¼Œä½†å›ç­”è¦æ¸…æ™°å®Œæ•´ã€ä¸è¦å †ç Œæ‹Ÿå£°è¯ã€‚",
    "å¹´è½»äºº": "ã€å½“å‰å¯¹è¯å¯¹è±¡ï¼šå¹´è½»äººã€‘è¯·ç”¨è½»æ¾ã€è‡ªç„¶ã€åƒæœ‹å‹èŠå¤©çš„è¯­æ°”å›ç­”ï¼šç›´æ¥ä¸å•°å—¦ï¼Œå¯ä»¥å¸¦ä¸€ç‚¹æ—¥å¸¸æˆ–ç½‘ç»œç”¨è¯­ï¼Œä¿æŒå‹å¥½å’Œå…±é¸£ï¼ŒåƒåŒé¾„äººä¸€æ ·äº¤æµã€‚",
    "è€å¹´äºº": "ã€å°å›¢å›¢å¯¹é•¿è¾ˆã€‘ä¿æŒå°Šæ•¬ä½“è´´ï¼ŒæŠŠè¯è¯´æ¸…æ¥šï¼Œå¤šç”¨æ•¬è¯­ï¼Œå°‘ç”¨ç½‘ç»œç”¨è¯­å’Œemojiï¼Œè®©å¯¹æ–¹æ„Ÿåˆ°è¢«å°Šé‡ã€‚è¯­æ°”ä»ä¿æŒæ¸©æš–ï¼Œä½†æ›´ç¨³é‡ã€‚",
}


def _build_chat_prompt(user_input: str) -> str:
    """GGUF ç­‰ä½¿ç”¨ï¼šUser/Assistant ç®€å•æ ¼å¼ã€‚"""
    tone = st.session_state.get("tone_style", "å¹´è½»äºº")
    instruction = TONE_PROMPTS.get(tone, "")
    if instruction:
        return f"{instruction}\n\nUser: {user_input}\nAssistant:"
    return f"User: {user_input}\nAssistant:"


# è¯­æ°”æ¥æºè¯´æ˜ï¼š
# - æ¨¡å‹ï¼ˆDPO æƒé‡ï¼‰ï¼šä» chosen/rejected å¯¹ä¸­å­¦åˆ°ã€Œæ´»æ³¼æ¸©æŸ” vs å®˜æ–¹ç”Ÿç¡¬ã€çš„åå¥½
# - Promptï¼ˆä¸‹æ–‡äººè®¾ï¼‰ï¼šä¸è®­ç»ƒæ—¶çš„ system ä¸€è‡´ï¼Œæä¾›ä¸Šä¸‹æ–‡ï¼Œä½¿æ¨¡å‹çŸ¥é“å½“å‰æ˜¯ã€Œå°å›¢å›¢ã€åœºæ™¯
# äºŒè€…ç¼ºä¸€ä¸å¯ï¼šæ—  prompt åˆ™æ¨¡å‹ä¸çŸ¥äººè®¾ï¼Œæ—  DPO åˆ™æ¨¡å‹æ— è¯¥åå¥½ã€‚å®é™…è¯­æ°” = ä¸¤è€…å…±åŒä½œç”¨ã€‚
DPO_SYSTEM_PROMPT = (
    "ä½ æ˜¯å°å›¢å›¢ï¼Œä¸€ä¸ªæ´»æ³¼æ¸©æŸ”ã€åƒæœ‹å‹ä¸€æ ·èŠå¤©çš„AIåŠ©æ‰‹ã€‚"
    "è¯·ç”¨è½»æ¾è‡ªç„¶çš„è¯­æ°”å›ç­”ï¼Œå¯å¸¦emojiå’Œç½‘ç»œç”¨è¯­ï¼Œé¿å…å®˜æ–¹ã€ç”Ÿç¡¬ã€æ¨¡æ¿åŒ–çš„è¡¨è¾¾ã€‚"
)

def _build_hf_prompt(user_input: str, tokenizer) -> str:
    """
    HF/DPO æ¨¡å‹ä¸“ç”¨ï¼šä½¿ç”¨ä¸ DPO è®­ç»ƒç›¸åŒçš„ chat_template å’Œ system äººè®¾ï¼Œ
    å¦åˆ™æ¨¡å‹åœ¨æ¨ç†æ—¶çœ‹åˆ°çš„æ ¼å¼ä¸è®­ç»ƒä¸ä¸€è‡´ï¼Œæ— æ³•æ­£ç¡®è¾“å‡ºæ´»æ³¼è¯­æ°”ã€‚
    """
    tone = st.session_state.get("tone_style", "å¹´è½»äºº")
    instruction = TONE_PROMPTS.get(tone, "")
    is_dpo = st.session_state.get("current_model") == "DPOå¾®è°ƒæ¨¡å‹"

    # DPO æ¨¡å‹ï¼šå¿…é¡»æ³¨å…¥è®­ç»ƒæ—¶çš„äººè®¾ï¼›æœ‰è¯­æ°”é€‰é¡¹æ—¶ä¸äººè®¾åˆå¹¶
    if is_dpo:
        system_content = DPO_SYSTEM_PROMPT
        if instruction:
            system_content = f"{DPO_SYSTEM_PROMPT}\n\n{instruction}"
        messages = [{"role": "system", "content": system_content}, {"role": "user", "content": user_input}]
    elif instruction:
        messages = [{"role": "system", "content": instruction}, {"role": "user", "content": user_input}]
    else:
        messages = [{"role": "user", "content": user_input}]
    try:
        prompt = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True,
        )
        return prompt
    except Exception:
        return _build_chat_prompt(user_input)


# åŠ è½½æ¨¡å‹ï¼ˆé¡»åœ¨ä¾§è¾¹æ â€œé¢„åŠ è½½æ¨¡å‹â€æŒ‰é’®ä¹‹å‰å®šä¹‰ï¼‰
@st.cache_resource
def load_model(model_type):
    """åŠ è½½æ¨¡å‹ã€‚æœ¬åœ°å¾®è°ƒï¼šä¼˜å…ˆ GGUF å•æ–‡ä»¶ï¼Œå¦åˆ™ HF ç›®å½•ï¼›ä¸åŠ è½½åŸºç¡€æ¨¡å‹ã€‚"""
    max_seq_length = 4096
    _log(f"å¼€å§‹åŠ è½½æ¨¡å‹ï¼Œç±»å‹: {model_type}")
    with st.spinner("ğŸ”„ æ­£åœ¨åŠ è½½æ¨¡å‹ï¼Œè¯·ç¨å€™..."):
        try:
            if model_type == "æœ¬åœ°å¾®è°ƒæ¨¡å‹":
                # 1) ä¼˜å…ˆï¼šGGUF å•æ–‡ä»¶
                for path in LOCAL_GGUF_FILES:
                    if not os.path.isfile(path):
                        _log(f"GGUF ä¸å­˜åœ¨ï¼Œè·³è¿‡: {path}")
                        continue
                    _log(f"å°è¯•åŠ è½½ GGUF: {path}")
                    try:
                        from llama_cpp import Llama
                        llm = Llama(
                            model_path=path,
                            n_ctx=max_seq_length,
                            n_gpu_layers=-1,
                            verbose=False,
                        )
                        _log(f"GGUF åŠ è½½æˆåŠŸ: {path}")
                        # æ£€æµ‹æ˜¯å¦ä¸º CUDA ç‰ˆï¼ˆå½“å‰è‹¥ä¸º CPU ç‰ˆä¼šå¯¼è‡´ GPU 0%ã€CPU 100% å¾ˆæ…¢ï¼‰
                        try:
                            from llama_cpp.llama_cpp import _load_shared_library
                            _lib = _load_shared_library("llama")
                            if getattr(_lib, "llama_supports_gpu_offload", lambda: False)():
                                _log("å½“å‰ llama-cpp-python æ”¯æŒ GPU å¸è½½ï¼Œæ¨ç†åº”èµ° GPU")
                            else:
                                _log("å½“å‰ä¸º CPU ç‰ˆ llama-cpp-pythonï¼Œæ¨ç†ä¼šéå¸¸æ…¢ï¼›è¯·å®‰è£… CUDA ç‰ˆï¼ˆè§ READMEï¼‰")
                                st.warning("âš ï¸ å½“å‰ä¸º **CPU ç‰ˆ** llama-cpp-pythonï¼Œæ¨ç†æ—¶ GPU ä¼šæ˜¾ç¤º 0%ã€CPU æ»¡è´Ÿè½½å¾ˆæ…¢ã€‚è¯·å®‰è£… CUDA ç‰ˆåé‡å¯åº”ç”¨ï¼Œè§ READMEã€Œè®© GGUF ä½¿ç”¨ 5090 æ˜¾å¡ã€ã€‚")
                        except Exception:
                            pass
                        st.success(f"âœ… æœ¬åœ° GGUF æ¨¡å‹åŠ è½½æˆåŠŸï¼š{path}")
                        return llm, None, "gguf"
                    except ImportError as e:
                        _log(f"ImportError: {e}")
                        st.error("âŒ è¯·å…ˆå®‰è£… llama-cpp-pythonï¼špip install llama-cpp-pythonï¼ˆGPU ç‰ˆéœ€å¸¦ CUDA ç¼–è¯‘ï¼‰")
                        return None, None, None
                    except Exception as e:
                        err = str(e)
                        _log(f"GGUF åŠ è½½å¼‚å¸¸ {path}: {err}")
                        if "not within the file bounds" in err or "corrupted or incomplete" in err:
                            st.warning(f"âš ï¸ GGUF æ–‡ä»¶å·²æŸåæˆ–ä¸å®Œæ•´ï¼Œè¯·é‡æ–°å¯¼å‡ºæˆ–ä¸‹è½½ï¼š{path}")
                        else:
                            st.warning(f"âš ï¸ GGUF {path} åŠ è½½å¤±è´¥: {err}")
                        continue
                # 2) å¤‡é€‰ï¼šHF æ ¼å¼ç›®å½•ï¼ˆéœ€ GPU + Unslothï¼‰
                for path in LOCAL_MODEL_DIRS:
                    if not os.path.exists(path):
                        _log(f"HF ç›®å½•ä¸å­˜åœ¨ï¼Œè·³è¿‡: {path}")
                        continue
                    _log(f"å°è¯•åŠ è½½ HF ç›®å½•: {path}")
                    try:
                        from unsloth import FastLanguageModel
                        model, tokenizer = FastLanguageModel.from_pretrained(
                            path,
                            max_seq_length=max_seq_length,
                            load_in_4bit=False,
                        )
                        model = FastLanguageModel.for_inference(model)
                        _log(f"HF åŠ è½½æˆåŠŸ: {path}")
                        st.success(f"âœ… æœ¬åœ°å¾®è°ƒæ¨¡å‹åŠ è½½æˆåŠŸï¼š{path}")
                        return model, tokenizer, "hf"
                    except Exception as e:
                        _log(f"HF åŠ è½½å¤±è´¥ {path}: {e}")
                        st.warning(f"âš ï¸ {path} åŠ è½½å¤±è´¥: {str(e)}")
                        continue
                _log("æœªæ‰¾åˆ°å¯ç”¨çš„æœ¬åœ°æ¨¡å‹")
                st.error("âŒ æœªæ‰¾åˆ°å¯ç”¨çš„æœ¬åœ°æ¨¡å‹ï¼ˆGGUF æ–‡ä»¶æˆ– merged_model ç­‰ç›®å½•ï¼‰ã€‚")
                return None, None, None

            if model_type == "DPOå¾®è°ƒæ¨¡å‹":
                path = LOCAL_DPO_MODEL_DIR
                if not os.path.isdir(path):
                    _log(f"DPO æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {path}")
                    st.error(f"âŒ DPO æ¨¡å‹ç›®å½•ä¸å­˜åœ¨ï¼š{path}\nè¯·å…ˆè¿è¡Œ `python train_dpo.py` å®Œæˆ DPO è®­ç»ƒã€‚")
                    return None, None, None
                _log(f"å°è¯•åŠ è½½ DPO æ¨¡å‹ï¼ˆåŸºç¡€æ¨¡å‹ + LoRA é€‚é…å™¨ï¼‰: {path}")
                try:
                    from unsloth import FastLanguageModel
                    model, tokenizer = FastLanguageModel.from_pretrained(
                        path,
                        max_seq_length=max_seq_length,
                        load_in_4bit=True,  # ä¸è®­ç»ƒæ—¶åŸºåº§ä¸€è‡´ï¼ŒèŠ‚çœæ˜¾å­˜
                    )
                    model = FastLanguageModel.for_inference(model)
                    _log(f"DPO æ¨¡å‹åŠ è½½æˆåŠŸ: {path}")
                    st.success(f"âœ… DPO å¾®è°ƒæ¨¡å‹åŠ è½½æˆåŠŸï¼š{path}")
                    return model, tokenizer, "hf"
                except Exception as e:
                    _log(f"DPO æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                    st.error(f"âŒ DPO æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
                    return None, None, None

            # åŸºç¡€æ¨¡å‹ï¼šä» HuggingFace åŠ è½½ï¼ˆéœ€ GPU + Unslothï¼‰
            _log("åŠ è½½åŸºç¡€æ¨¡å‹ï¼ˆHuggingFaceï¼‰...")
            try:
                from unsloth import FastLanguageModel
                model_name = "unsloth/deepseek-r1-distill-qwen-7b-unsloth-bnb-4bit"
                model, tokenizer = FastLanguageModel.from_pretrained(
                    model_name=model_name,
                    max_seq_length=max_seq_length,
                    load_in_4bit=True,
                )
                model = FastLanguageModel.for_inference(model)
                _log("åŸºç¡€æ¨¡å‹åŠ è½½æˆåŠŸ")
                st.success("âœ… åŸºç¡€æ¨¡å‹åŠ è½½æˆåŠŸï¼")
                return model, tokenizer, "hf"
            except Exception as e:
                err = str(e)
                if "torch accelerator" in err.lower() or "need a gpu" in err.lower() or "cuda" in err.lower():
                    _log(f"åŸºç¡€æ¨¡å‹åŠ è½½å¤±è´¥ï¼ˆPyTorch æœªè§ GPUï¼‰: {e}")
                    cuda_visible = torch.cuda.is_available()
                    st.error(
                        "âŒ åŸºç¡€æ¨¡å‹ä¾èµ– **PyTorch** çš„ GPUï¼Œå½“å‰ PyTorch æœªæ£€æµ‹åˆ°å¯ç”¨ GPUã€‚"
                        "ï¼ˆGGUF ç”¨çš„æ˜¯ llama-cpp-python çš„ CUDAï¼Œå’Œ PyTorch æ— å…³ï¼Œæ‰€ä»¥å¾®è°ƒæ¨¡å‹èƒ½è·‘ã€‚ï¼‰\n\n"
                        "**è§£å†³åŠæ³•**ï¼šå®‰è£… PyTorch çš„ CUDA ç‰ˆåå†è¯•ã€‚AutoDL å¸¸è§ä¸º CUDA 12.8ï¼š\n"
                        "`pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128`"
                    )
                    return None, None, None
                raise

        except Exception as e:
            _log(f"æ¨¡å‹åŠ è½½å¼‚å¸¸: {e}")
            st.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
            return None, None, None


# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(
    page_title="AIé™ªä¼´æœºå™¨äºº",
    page_icon="ğŸ¤–",
    layout="wide"
)

# ç¡®ä¿ä¸»å†…å®¹åŒºå¯æ­£å¸¸ç”¨æ»šè½®ä¸Šä¸‹æ»šåŠ¨ï¼ˆä¸é”æ­»åº•éƒ¨ï¼‰
st.markdown(
    """
    <style>
    [data-testid="stAppViewContainer"] { overflow-y: auto !important; }
    .main .block-container { overflow-y: visible !important; }
    </style>
    """,
    unsafe_allow_html=True,
)

# å°½æ—©åˆå§‹åŒ– load_statusï¼Œé¿å…ä¸‹æ–¹ caption æŠ¥ AttributeError
if "load_status" not in st.session_state:
    st.session_state.load_status = "æœªåŠ è½½ï¼ˆå‘é€æ¶ˆæ¯æˆ–ç‚¹å‡»ã€Œé¢„åŠ è½½æ¨¡å‹ã€ååŠ è½½ï¼‰"

st.title("ğŸ¤– AIé™ªä¼´æœºå™¨äºº")
# æ¨¡å‹çŠ¶æ€ï¼ˆä¸»åŒºåŸŸæ˜¾ç¤ºï¼Œé¿å…æµè§ˆå™¨ AbortSignal ç­‰å¯¼è‡´ä¾§æ æç¤ºä¸æ˜¾ç¤ºï¼‰
st.caption("ğŸ“Œ æ¨¡å‹çŠ¶æ€ï¼š**" + st.session_state.load_status + "**")
st.markdown("---")

# ä¾§è¾¹æ é…ç½®
with st.sidebar:
    st.header("âš™ï¸ æ¨¡å‹é…ç½®")
    
    # è¯­æ°”é£æ ¼ï¼ˆå¯¹è¯å¯¹è±¡ï¼‰ï¼šå°å­© / å¹´è½»äºº / è€å¹´äºº
    tone_style = st.selectbox(
        "è¯­æ°”é£æ ¼ï¼ˆå¯¹è¯å¯¹è±¡ï¼‰",
        options=["æ— æç¤ºè¯", "å°å­©", "å¹´è½»äºº", "è€å¹´äºº"],
        index=0,
        key="tone_style",
        help="ã€Œæ— æç¤ºè¯ã€= ä¸åŠ ä»»ä½•è¯­æ°”æŒ‡ä»¤ï¼ŒæŸ¥çœ‹æ¨¡å‹åŸå§‹æ•ˆæœï¼›å…¶ä½™é€‰é¡¹ä¼šæ³¨å…¥å¯¹åº”è¯­æ°”æç¤ºè¯ã€‚",
    )
    
    # PyTorch æ˜¯å¦å¯è§ GPUï¼ˆåŸºç¡€æ¨¡å‹ä¾èµ– PyTorchï¼›GGUF ç”¨ llama-cpp-python çš„ CUDAï¼ŒäºŒè€…ç‹¬ç«‹ï¼‰
    _cuda_ok = torch.cuda.is_available()
    _cuda_msg = f"æ˜¯ï¼ˆ{torch.cuda.get_device_name(0)}ï¼‰" if _cuda_ok else "å¦"
    st.caption(f"ğŸ”§ PyTorch å¯è§ GPU: **{_cuda_msg}**")
    if not _cuda_ok:
        st.caption("ç³»ç»Ÿæœ‰ CUDA ä½† PyTorch å¯èƒ½æ˜¯ CPU ç‰ˆï¼Œéœ€é‡è£… cu128 ç‰ˆï¼›GGUF ä¸å—å½±å“")
    
    # æ¨¡å‹é€‰æ‹©
    model_choice = st.selectbox(
        "é€‰æ‹©æ¨¡å‹åŠ è½½æ–¹å¼",
        ["æœ¬åœ°å¾®è°ƒæ¨¡å‹", "DPOå¾®è°ƒæ¨¡å‹", "åŸºç¡€æ¨¡å‹"],
        help="æœ¬åœ°å¾®è°ƒæ¨¡å‹=SFTè®­ç»ƒäº§å‡ºï¼›DPOå¾®è°ƒæ¨¡å‹=DPOè®­ç»ƒäº§å‡ºï¼ˆè§£å†³é‡å¤/æˆªæ–­é—®é¢˜ï¼‰ï¼›åŸºç¡€æ¨¡å‹=åŸå§‹é¢„è®­ç»ƒã€‚",
    )
    if model_choice == "åŸºç¡€æ¨¡å‹":
        st.caption("âš ï¸ éœ€ PyTorch èƒ½è§ GPUï¼Œå¦åˆ™ä¼šåŠ è½½å¤±è´¥")
    elif model_choice == "DPOå¾®è°ƒæ¨¡å‹":
        _dpo_exists = os.path.isdir(LOCAL_DPO_MODEL_DIR)
        st.caption(f"{'âœ…' if _dpo_exists else 'âŒ'} lora_model_dpo/ {'å·²å°±ç»ª' if _dpo_exists else 'ä¸å­˜åœ¨ï¼Œè¯·å…ˆè¿è¡Œ train_dpo.py'}")
    
    # ç”Ÿæˆå‚æ•°é…ç½®
    col1, col2 = st.columns(2)
    with col1:
        max_tokens = st.slider(
            "æœ€å¤§ç”Ÿæˆé•¿åº¦",
            min_value=50,
            max_value=2048,
            value=500,
            step=50
        )
    
    with col2:
        temperature = st.slider(
            "æ¸©åº¦(Temperature)",
            min_value=0.0,
            max_value=2.0,
            value=0.7,
            step=0.1
        )
    
    top_p = st.slider(
        "Top P",
        min_value=0.0,
        max_value=1.0,
        value=0.9,
        step=0.05
    )
    
    st.markdown("---")
    # é¢„åŠ è½½æŒ‰é’®ï¼šå…ˆåŠ è½½æ¨¡å‹å¹¶çœ‹ç»ˆç«¯æ—¥å¿—ï¼Œé¿å…â€œå‘æ¶ˆæ¯æ‰åŠ è½½â€ä¸”å‰ç«¯ä¸æ˜¾ç¤ºçš„é—®é¢˜
    if st.button("ğŸ”„ é¢„åŠ è½½æ¨¡å‹", use_container_width=True, type="primary"):
        _log(f"ç”¨æˆ·ç‚¹å‡»é¢„åŠ è½½ï¼Œå½“å‰é€‰æ‹©: {model_choice}")
        model, tokenizer, backend = load_model(model_choice)
        st.session_state.model = model
        st.session_state.tokenizer = tokenizer
        st.session_state.backend = backend or "hf"
        st.session_state.model_loaded = True
        st.session_state.current_model = model_choice
        if model is not None:
            st.session_state.load_status = f"âœ… å·²åŠ è½½ï¼ˆ{model_choice}ï¼‰"
            _log("é¢„åŠ è½½æˆåŠŸ")
        else:
            st.session_state.load_status = "âŒ åŠ è½½å¤±è´¥ï¼Œè¯·æŸ¥çœ‹ç»ˆç«¯æ—¥å¿—"
            _log("é¢„åŠ è½½å¤±è´¥")
    st.markdown("---")
    st.markdown("**ğŸ’¡ å‚æ•°è¯´æ˜:**")
    st.markdown("""
    - **æ¸©åº¦**: å€¼è¶Šé«˜è¶Šå…·æœ‰åˆ›æ„ï¼Œè¶Šä½è¶Šä¿å®ˆ
    - **Top P**: æ§åˆ¶ç”Ÿæˆå¤šæ ·æ€§çš„å‚æ•°
    """)


# åˆå§‹åŒ–session state
if "messages" not in st.session_state:
    st.session_state.messages = []
    st.session_state.model_loaded = False
    st.session_state.model = None
    st.session_state.tokenizer = None
    st.session_state.current_model = None
    st.session_state.backend = "hf"  # "hf" | "gguf"


def _strip_think_tags(text: str) -> str:
    """åªä¿ç•™ </think> ä¹‹åçš„å›ç­”éƒ¨åˆ†ï¼Œå»æ‰ <think>...</think> æ€è€ƒå†…å®¹åŠæœ«å°¾è‡ªæ£€å¥ï¼ˆå¦‚ã€Œè¯·é—®ï¼Œè¿™ä¸ªå›åº”æ˜¯å¦ç¬¦åˆè¦æ±‚ï¼Ÿã€ï¼‰ã€‚"""
    if not text:
        return ""
    # æŒ‰ç¬¬ä¸€ä¸ª </think> æˆªæ–­ï¼Œåªä¿ç•™åé¢çš„æ­£å¼å›ç­”
    if "</think>" in text:
        out = text.split("</think>", 1)[-1].strip()
    else:
        out = re.sub(r"<think>.*", "", text, flags=re.DOTALL).strip()
    # è‹¥å›ç­”é‡Œåˆå‡ºç° <think>ï¼Œåªä¿ç•™ç¬¬ä¸€æ®µï¼ˆåˆ°ä¸‹ä¸€ä¸ª <think> æˆ– </think> ä¹‹å‰ï¼‰
    if "<think>" in out:
        out = re.sub(r"<think>.*", "", out, flags=re.DOTALL).strip()
    # å»æ‰æœ«å°¾æ¨¡å‹è‡ªæ£€å¥
    out = re.sub(r"è¯·é—®[ï¼Œ,]?\s*è¿™ä¸ªå›åº”æ˜¯å¦ç¬¦åˆè¦æ±‚[ï¼Ÿ?].*$", "", out, flags=re.DOTALL).strip()
    return out


def stream_gguf_response(user_input, placeholder, max_tok, temp, top_p_val):
    """GGUF æµå¼ç”Ÿæˆï¼Œè¾¹ç”Ÿæˆè¾¹æ›´æ–° placeholderï¼Œè¿”å›å®Œæ•´å›å¤ã€‚åªæ˜¾ç¤º </think> åçš„å›ç­”ã€‚"""
    prompt = _build_chat_prompt(user_input)
    full = ""
    try:
        stream = st.session_state.model(
            prompt,
            max_tokens=max_tok,
            temperature=temp,
            top_p=top_p_val,
            repeat_penalty=1.15,
            stop=["User:", "\nUser:", "å°å›¢å›¢'ã€‚ã€‚"],
            echo=False,
            stream=True,
        )
        for chunk in stream:
            piece = (chunk.get("choices") or [{}])[0].get("text") or ""
            full += piece
            # åªæŠŠ </think> åçš„å†…å®¹å±•ç¤ºç»™ç”¨æˆ·ï¼Œé¿å…éœ²å‡ºæ€è€ƒè¿‡ç¨‹
            to_show = _strip_think_tags(full)
            placeholder.markdown(to_show + "â–Œ")
        to_show = _strip_think_tags(full)
        placeholder.markdown(to_show)
        if "Assistant:" in full:
            full = full.split("Assistant:")[-1].strip()
        full = _strip_think_tags(full)
        return full.strip() or "ï¼ˆæ— è¾“å‡ºï¼‰"
    except Exception as e:
        placeholder.markdown(f"âŒ ç”Ÿæˆå¤±è´¥: {str(e)}")
        return f"âŒ ç”Ÿæˆå¤±è´¥: {str(e)}"


def stream_hf_response(user_input, placeholder, max_tok, temp, top_p_val):
    """HF æ¨¡å‹æµå¼ç”Ÿæˆï¼Œä½¿ç”¨ chat_template ä¸è®­ç»ƒæ ¼å¼ä¸€è‡´ï¼Œåªæ˜¾ç¤º </think> åçš„å›ç­”ã€‚"""
    if st.session_state.model is None or st.session_state.tokenizer is None:
        placeholder.markdown("âŒ æ¨¡å‹æœªåŠ è½½æˆ–çŠ¶æ€å¼‚å¸¸")
        return "âŒ æ¨¡å‹æœªåŠ è½½"
    try:
        from transformers import TextIteratorStreamer
        prompt = _build_hf_prompt(user_input, st.session_state.tokenizer)
        device = "cuda" if torch.cuda.is_available() else "cpu"
        inputs = st.session_state.tokenizer(
            prompt, return_tensors="pt", truncation=True, max_length=4096
        ).to(device)
        streamer = TextIteratorStreamer(
            st.session_state.tokenizer, skip_special_tokens=True
        )
        gen_kwargs = dict(
            **inputs,
            max_new_tokens=max_tok,
            temperature=temp,
            top_p=top_p_val,
            do_sample=True,
            repetition_penalty=1.15,  # æŠ‘åˆ¶å‘€å‘€å‘€ã€æŠ±æŠ±æŠ±ç­‰é‡å¤
            pad_token_id=st.session_state.tokenizer.pad_token_id,
            eos_token_id=st.session_state.tokenizer.eos_token_id,
            streamer=streamer,
        )
        thread = Thread(target=st.session_state.model.generate, kwargs=gen_kwargs)
        thread.start()
        full = ""
        for new_text in streamer:
            full += new_text
            placeholder.markdown(_strip_think_tags(full) + "â–Œ")
        thread.join()
        full = _strip_think_tags(full)
        placeholder.markdown(full)
        if "Assistant:" in full:
            full = full.split("Assistant:")[-1].strip()
        full = _strip_think_tags(full)
        return full.strip() or "ï¼ˆæ— è¾“å‡ºï¼‰"
    except Exception as e:
        placeholder.markdown(f"âŒ ç”Ÿæˆå¤±è´¥: {str(e)}")
        return f"âŒ ç”Ÿæˆå¤±è´¥: {str(e)}"


def generate_response(user_input):
    """ç”Ÿæˆæ¨¡å‹å“åº”ï¼ˆæ”¯æŒ HF ä¸ GGUF ä¸¤ç§åç«¯ï¼‰"""
    if st.session_state.model is None:
        return "âŒ æ¨¡å‹æœªåŠ è½½ï¼Œè¯·å…ˆåŠ è½½æ¨¡å‹"
    backend = st.session_state.get("backend", "hf")

    try:
        prompt = (_build_hf_prompt(user_input, st.session_state.tokenizer)
                  if st.session_state.tokenizer else _build_chat_prompt(user_input))

        if backend == "gguf":
            # GGUF ä½¿ç”¨æµå¼åœ¨å¤–éƒ¨è°ƒç”¨ï¼Œæ­¤å¤„ä»…ä½œéæµå¼å…œåº•ï¼ˆä¸€èˆ¬ä¸èµ°åˆ°ï¼‰
            with st.spinner("ğŸ¤” æ¨¡å‹æ€è€ƒä¸­..."):
                out = st.session_state.model(
                    prompt,
                    max_tokens=min(max_tokens, 256),
                    temperature=temperature,
                    top_p=top_p,
                    repeat_penalty=1.15,
                    stop=["User:", "\nUser:", "å°å›¢å›¢'ã€‚ã€‚"],
                    echo=False,
                )
                response = (out["choices"][0].get("text") or "").strip()
                if "Assistant:" in response:
                    response = response.split("Assistant:")[-1].strip()
                response = _strip_think_tags(response)
                return response or "ï¼ˆæ— è¾“å‡ºï¼‰"

        # HF åç«¯ï¼šåªè§£ç æ–°ç”Ÿæˆéƒ¨åˆ†ï¼Œå¹¶å»é™¤ <think> å†…å®¹
        if st.session_state.tokenizer is None:
            return "âŒ æ¨¡å‹çŠ¶æ€å¼‚å¸¸"
        enc = st.session_state.tokenizer(prompt, return_tensors="pt")
        input_len = enc["input_ids"].shape[1]
        enc = {k: v.to("cuda" if torch.cuda.is_available() else "cpu") for k, v in enc.items()}
        with torch.no_grad():
            outputs = st.session_state.model.generate(
                **enc,
                max_new_tokens=max_tokens,
                temperature=temperature,
                top_p=top_p,
                do_sample=True,
                repetition_penalty=1.15,
                pad_token_id=st.session_state.tokenizer.pad_token_id,
                eos_token_id=st.session_state.tokenizer.eos_token_id,
            )
        response = st.session_state.tokenizer.decode(
            outputs[0][input_len:], skip_special_tokens=True
        )
        response = _strip_think_tags(response)
        if "Assistant:" in response:
            response = response.split("Assistant:")[-1].strip()
        return response.strip() or "ï¼ˆæ— è¾“å‡ºï¼‰"
    except Exception as e:
        return f"âŒ ç”Ÿæˆå¤±è´¥: {str(e)}"


# ä¸»ç•Œé¢
col1, col2 = st.columns([3, 1])

with col1:
    st.subheader("ğŸ’¬ å¯¹è¯åŒºåŸŸ")

with col2:
    if st.button("ğŸ”„ æ¸…ç©ºå¯¹è¯", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

# æ˜¾ç¤ºå¯¹è¯å†å²
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ã€Œå›åˆ°åº•éƒ¨ã€æŒ‰é’®ï¼šä»…ç‚¹å‡»æ—¶æ‰§è¡Œä¸€æ¬¡æ»šåŠ¨ï¼Œä¸å¹²æ‰°å¹³æ—¶ç”¨æ»šè½®ä¸Šä¸‹ç¿»çœ‹
if st.session_state.get("do_auto_scroll", False):
    try:
        from streamlit.components.v1 import html as st_html
        st_html(
            """
            <script>
            (function() {
                var t = window.parent;
                if (!t || t === window) t = window;
                function run() {
                    try {
                        var app = t.document.querySelector('[data-testid="stAppViewContainer"]');
                        if (app && app.scrollHeight > app.clientHeight) {
                            app.scrollTop = app.scrollHeight;
                        }
                        var main = t.document.querySelector('.main');
                        if (main && main.scrollHeight > main.clientHeight) {
                            main.scrollTop = main.scrollHeight;
                        }
                        t.scrollTo(0, t.document.body.scrollHeight);
                    } catch (e) {}
                }
                setTimeout(run, 100);
            })();
            </script>
            """,
            height=0,
        )
    except Exception:
        pass
    st.session_state.do_auto_scroll = False

with col2:
    if st.button("â¬‡ï¸ å›åˆ°åº•éƒ¨", use_container_width=True, help="æ»šåŠ¨åˆ°æœ€æ–°ä¸€æ¡æ¶ˆæ¯"):
        st.session_state.do_auto_scroll = True
        st.rerun()

# è¾“å…¥æ¡†
user_input = st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜æˆ–æƒ³è¯´çš„è¯...")

if user_input:
    # åŠ è½½æ¨¡å‹ï¼ˆå¦‚æœè¿˜æœªåŠ è½½ï¼‰
    if not st.session_state.model_loaded or st.session_state.current_model != model_choice:
        model, tokenizer, backend = load_model(model_choice)
        st.session_state.model = model
        st.session_state.tokenizer = tokenizer
        st.session_state.backend = backend or "hf"
        st.session_state.model_loaded = True
        st.session_state.current_model = model_choice
        st.session_state.load_status = f"âœ… å·²åŠ è½½ï¼ˆ{model_choice}ï¼‰" if model else "âŒ åŠ è½½å¤±è´¥ï¼Œè¯·æŸ¥çœ‹ç»ˆç«¯"
    
    # æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯
    st.session_state.messages.append({
        "role": "user",
        "content": user_input
    })
    
    with st.chat_message("user"):
        st.markdown(user_input)
    
    # ç”ŸæˆAIå“åº”
    if st.session_state.model is not None:
        backend = st.session_state.get("backend", "hf")
        if backend == "gguf":
            with st.chat_message("assistant"):
                placeholder = st.empty()
                response = stream_gguf_response(
                    user_input, placeholder,
                    max_tok=min(max_tokens, 512),
                    temp=temperature,
                    top_p_val=top_p,
                )
        else:
            # HF åŸºç¡€æ¨¡å‹ä¹Ÿæµå¼è¾“å‡º
            with st.chat_message("assistant"):
                placeholder = st.empty()
                response = stream_hf_response(
                    user_input, placeholder,
                    max_tok=min(max_tokens, 512),
                    temp=temperature,
                    top_p_val=top_p,
                )
        
        # ä¿å­˜åˆ°å¯¹è¯å†å²
        st.session_state.messages.append({
            "role": "assistant",
            "content": response
        })
    else:
        st.error("âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œæ— æ³•ç”Ÿæˆå“åº”")
    
    st.rerun()

# åº•éƒ¨ä¿¡æ¯
st.markdown("---")
st.markdown("""
<div style='text-align: center'>
    <small>ğŸš€ é©±åŠ¨æ¨¡å‹: DeepSeek R1 Distill Qwen 7B</small>
    <br>
    <small>âš¡ æ¡†æ¶: Unsloth + Streamlit</small>
</div>
""", unsafe_allow_html=True)
